{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science - Homework 8\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "Due: Friday, March 16, 11:59pm.\n",
    "\n",
    "In this homework, you will use clustering, regular expressions, and natural language processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Data\n",
    "First Name:Travis\n",
    "<br>\n",
    "Last Name:Tiner\n",
    "<br>\n",
    "E-mail:u0769566@utah.edu\n",
    "<br>\n",
    "UID:u0769566\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Analyze US Crime data\n",
    "\n",
    "We'll analyze a dataset describing 1973 violent crime rates by US State. The crimes considered are assault, murder, and rape. Also included is the percent of the population living in urban areas.\n",
    "\n",
    "The dataset is available as *USarrests.csv*. The dataset has 50 observations (corresponding to each state) on 4 variables: \n",
    "1. Murder: Murder arrests (per 100,000 residents)\n",
    "2. Assault: Assault arrests (per 100,000 residents)\n",
    "3. UrbanPop: Percent urban population\n",
    "4. Rape: Rape arrests (per 100,000 residents)\n",
    "\n",
    "\n",
    "You can read more about the dataset [here](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/USArrests.html). \n",
    "\n",
    "Our goal will be to use clustering tools to understand how violent crimes differ between states. \n",
    "\n",
    "\n",
    "### Task 1.1 Import the data and perform some prelimary exploratory analysis. \n",
    "Use the *read_csv* pandas function to import the data as a dataframe. \n",
    "\n",
    "Plot a scatterplot matrix of the data. Explore basic statistics of the data. Write a few sentences describing how the variables are correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your description:** TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 k-means cluster analysis\n",
    "1. Scale the dataset using the *scale* function of the sklearn.preprocessing library. \n",
    "+ Using k-means, cluster the states into four clusters. Which states belong to which clusters?\n",
    "+ Vary k and find the *best* value. How do you determine *best*? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 Hierarchical cluster analysis\n",
    "\n",
    "1.  Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states into four clusters. Which states belong to which clusters? \n",
    "2. Do you get similiar results as for k-means? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Regular Expressions \n",
    "\n",
    "Write regular expressions for the following examples that matches the data of the given format and any other reasonable variations thereof. E.g., your regex shouldn't be specific to one URL or one phone number, but should work for all examples of the same format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.** Writes a regular expression that extracts the urls out of this string, but only the URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"To learn about pros/cons of data science, go to http://datascience.net.\\\n",
    "Alternatively, go to datascience.net/2018/\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://datascience.net', 'datascience.net/2018/']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[\\w:/\\w]+[.][\\w/\\-:]+', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.** Write a regular expression that extracts all phone numbers and fax numbers from this text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"You can reach me at 801-774-4321, or my office at (801) 223 9572.\\ \n",
    "Send me a fax at 857 188 7422. We finally made the sale for all 977 giraffes.\\\n",
    "They wanted 225 957 dollars for it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['801-774-4321', '857 188 7422']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[0-9]{3}[\\s-][0-9]{3}[\\s-][0-9]{4}',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['801-774-4321', '(801) 223 9572', '857 188 7422']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\(?\\d{3}\\)?[\\s-]?\\d{3}[\\s-]?\\d{4}',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.** Write a regular expression that extracts all opening html tags from this, including `<br />`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = \"This is <b>important</b> and <u>very</u><i>timely</i><br />. Was this <span> what you meant?</span>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<b>', '<u>', '<i>', '<br />', '<span>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\<\\w+\\s?\\/?\\>',html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.** Write a regular expression that extracts all the names of people from the following text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Arnold Schwarzenegger was born in Austria. He and Sylvester Stalone used to run a restaurant\\\n",
    "with J. Edgar Hoover.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arnold Schwarzenegger', ' Sylvester Stalone', 'J. Edgar Hoover']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'([A-Z]?\\.?\\s?[A-Z][a-z]+\\s[A-Z][a-z]+)',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5.** Write a regular expression that extracts the text out of all html elements of class important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Lorem ipsum dolor <b>sit</b> amet, <b class=\"important\">consectetur adipiscing</b> elit,\\ \n",
    "sed do eiusmod <span id=\"note\">tempor incididunt ut</span> <div>labore <strong class=important>\\\n",
    "et dolore magna</strong> aliqua.</div> Ut enim ad minim veniam, quis nostrud exercitation ullamco.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP: Classifying Newsgroups Articles\n",
    "\n",
    "Newsgroups were the social media of the 90s. Newsgroups are open discussion forums structured into hierarchices. For example, the following newsgroups cover topics as divers as atheism, computer graphics, and classified ads.  \n",
    "\n",
    "```\n",
    "alt.atheism\n",
    "comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\n",
    "misc.forsale\n",
    "```\n",
    "\n",
    "We will be combining machine learning and natural language processing to classify the news articles into these groups. We expect, for example, that the text for a classified ad in `misc.forsale` is different from text in `alt.atheism`. \n",
    "\n",
    "We will use the 20 Newsgroups corpus from scikit-learn. The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics. The general steps we follow are:\n",
    "1. Load the corpus    \n",
    "+ Do preprocessing: removal of stopwords, stemming, etc.\n",
    "+ Vectorize the text\n",
    "+ Split into training and test sets\n",
    "+ Train our classifier\n",
    "\n",
    "Refer to documentation on the [20 newsgroups dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) to learn about the dataset and find out how to download it.\n",
    "We recommended you use the `subset='all'` parameter to load all the data at once, instead of `subset='train'` and `subset='test'` seperately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.** Load the dataset.\n",
    "\n",
    "1. Print the exact number of news articles in the corpus.\n",
    "2. Print all 20 categories the news articles belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Classification\n",
    "\n",
    "Vectorize the data using vectorizers from sklearn. Using these vectors as features and the article category from corpus as labels, train a NaiveBayes classifer to classify the data.\n",
    "\n",
    "#### Vectorizers\n",
    "\n",
    "Vectorizes help us to transform text data into features we can use in machine learning. We did the vectorization manually in class, here you will use pre-build vectorizers. \n",
    "\n",
    "You should use CountVectorizer and TfidfVectorizer vectorizers from sklearn to vectorize your data. Please refer to documentation on both to learn how to use them.\n",
    "+ http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "+ http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "Compare the performance of classifiers using both vectorizers. You are encouraged to experiment with different parameters like max_df, min_df, etc. See docs for the meanings.\n",
    "\n",
    "#### Naive Bayes\n",
    "**Resources**\n",
    "1. https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "2. https://www.geeksforgeeks.org/naive-bayes-classifiers\n",
    "3. http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "We will be using Multinomial Naive Bayes from sklearn. Refer to documentation above for how to import the classifer. Then it can be used like any other classifer by using fit and predict functions provided on it.\n",
    "e.g:\n",
    "\n",
    "```\n",
    "clf = MulitnomialNB(alpha = 1)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "```\n",
    "\n",
    "Alpha is also known as the smoothing factor and ranges from 0 (no smoothing) to 1 (Laplace Smoothing). You can experiment with different values to see if you get better results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords\n",
    "\n",
    "Now we'll use the NLTK stopword list to improve our data vectors. TfidfVectorizer and CountVectorizer both can take an argument called stop_words. The words passed to this arguement are considered as stopwords and are not vectorized. Use the stopwords list from NLTK and pass it to vectorizers. Then evalulate the new vectors using Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "1. How much accuracy would a naive approach, that picks one of the 20 categories at random achieve?\n",
    "1. What accuracy where you able to achieve? \n",
    "1. What was the influence of the different vectorizers and the stopword removal? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Reponses:** TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
